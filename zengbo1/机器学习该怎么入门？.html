<meta name="referrer" content="no-referrer" />
<meta charset="utf-8" />
建议你跟着世界一流高校的一门课学习，而不要去看ng 的公开课。
<br>
stanford 一门ee xxx， machine learning，据说效果很好
<br>
berkeley有cs 289 A/B 供您选择
<br>
mit 想必也是有的
<br>
cmu 想必也是有的
<br>
<br>
这些课程都有网络版
<br>
<br>
教材的话，楼上有人说了，elements of statistical learning (esl)。这个说的高屋建瓴，适合数学基础好的人看；另外一本书是introduction to statistical learning with R。这个说的很实际，适合入门理解。
<br>
实际上书都写得很好，问题是你有多想学。你把他们都好好学会了，获得学校里面科班的训练，那你绝对比知乎上99％的半吊子水平要高了。剩下的就是学一学软件。
<br>
亚洲的精英教育鼓励大家追究刁钻的细节，却不清楚一门学问里最基本的普世的动机。却不知道，入门最基本的就是了解这个动机。刁钻的技术细节，一个理解动机的人很容易就能看懂
<br>
<br>
另附cs 289A 的内容大纲：
<br>
<br>
<ul>
	<li>
		Introduction: applications, methods, concepts
	</li>
	<li>
		Good Machine Learning hygiene: test/training/validation, overfitting
	</li>
	<li>
		Linear classification
	</li>
	<ul>
		<li>
			<b>
				<i>
					Perceptron
				</i>
			</b>
			 algorithm
		</li>
		<li>
			Support vector machines (
			<b>
				<i>
					SVMs
				</i>
			</b>
			)
		</li>
	</ul>
	<li>
		Statistical learning background
	</li>
	<ul>
		<li>
			Decision theory; Bayes risk
		</li>
		<li>
			Probabilistic models vs no model
		</li>
		<li>
			Generative and discriminative models
		</li>
		<li>
			Controlling complexity: regularization, bias-variance trade-off, priors.
		</li>
		<li>
			Resampling, cross-validation.
		</li>
		<li>
			The multivariate normal distribution.
		</li>
	</ul>
	<li>
		Linear regression
	</li>
	<ul>
		<li>
			<b>
				<i>
					Least squares
				</i>
			</b>
		</li>
		<li>
			Regularization: 
			<b>
				<i>
					ridge regression, lasso
				</i>
			</b>
		</li>
	</ul>
	<li>
		Brief primer on optimization
	</li>
	<li>
		Linear Classification, revisited
	</li>
	<ul>
		<li>
			<b>
				<i>
					Logistic regression
				</i>
			</b>
		</li>
		<li>
			<b>
				<i>
					Linear Discriminant Analysis
				</i>
			</b>
		</li>
		<li>
			Support vector machines revisited
		</li>
		<ul>
			<li>
				Algorithms
			</li>
			<li>
				<b>
					<i>
						The kernel trick
					</i>
				</b>
			</li>
		</ul>
	</ul>
	<li>
		Theoretical analysis of machine learning problems and algorithms
	</li>
	<ul>
		<li>
			Generalization error bounds; VC dimension
		</li>
	</ul>
	<li>
		Nearest neighbor methods
	</li>
	<ul>
		<li>
			<b>
				<i>
					k-nearest-neighbor
				</i>
			</b>
		</li>
		<li>
			Properties of high-dimensional spaces
		</li>
		<li>
			Distance learning
		</li>
		<li>
			Efficient indexing and retrieval methods
		</li>
	</ul>
	<li>
		Decision trees
	</li>
	<ul>
		<li>
			<i>
				<b>
					Classification and regression trees
				</b>
			</i>
		</li>
		<li>
			Random Forests
		</li>
		<li>
			<i>
				<b>
					Boosting
				</b>
			</i>
		</li>
	</ul>
	<li>
		Neural networks
	</li>
	<ul>
		<li>
			<i>
				<b>
					Multilayer perceptrons
				</b>
			</i>
		</li>
		<li>
			Variations such as convolutional nets
		</li>
		<li>
			Applications
		</li>
	</ul>
	<li>
		Unsupervised methods
	</li>
	<ul>
		<li>
			<i>
				<b>
					Clustering
				</b>
			</i>
		</li>
		<li>
			Density estimation
		</li>
		<li>
			Dimensionality reduction
		</li>
	</ul>
	<li>
		Applications in 
		<i>
			<b>
				Data Mining
			</b>
		</i>
	</li>
	<ul>
		<li>
			Collaborative filtering
		</li>
		<li>
			The power and the peril of Big Data 
		</li>
	</ul>
</ul>
